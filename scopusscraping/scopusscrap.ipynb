#!/usr/bin/env python3
"""
Fixed Birmingham Pipeline - Using complete field list to get author data
"""

import os
import json
import time
import random
import requests
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

class FixedBirminghamPipeline:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.elsevier.com/content/search/scopus"
        self.headers = {
            'Accept': 'application/json',
            'X-ELS-APIKey': api_key
        }
        # Complete field list like your working script - this is the key!
        self.field_list = (
            'dc:identifier,dc:title,dc:creator,dc:description,'
            'prism:publicationName,prism:coverDate,citedby-count,'
            'author,affiliation,authkeywords,prism:doi,eid,'
            'author-count,subject-area'
        )
        
        self.birmingham_affiliations = {
            'main': ('60019702', 'University of Birmingham'),
            'dubai': ('60293336', 'University of Birmingham Dubai'),
            'business': ('60172646', 'Birmingham Business School'),
            'medical_school': ('60172647', 'Birmingham Medical School'),
        }

    def search_papers(self, query, count=25, start=0):
        """Search with complete field list"""
        params = {
            'query': query,
            'count': count,
            'start': start,
            'field': self.field_list,
            'view': 'COMPLETE'  # This ensures we get full author data
        }
        
        try:
            time.sleep(random.uniform(0.3, 0.8))
            response = requests.get(self.base_url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"API Error: {e}")
            return None

    def extract_authors_correctly(self, paper):
        """Extract authors using the method that works with your successful data"""
        authors = []
        
        if 'author' in paper:
            author_list = paper['author'] if isinstance(paper['author'], list) else [paper['author']]
            
            for author in author_list:
                if isinstance(author, dict):
                    name = None
                    
                    # Try the field combinations that actually work
                    if author.get('authname'):
                        name = author['authname']
                    elif author.get('ce:indexed-name'):
                        name = author['ce:indexed-name']
                    elif author.get('surname') and author.get('given-name'):
                        name = f"{author['given-name']} {author['surname']}"
                    elif author.get('surname'):
                        name = author['surname']
                    elif author.get('given-name'):
                        name = author['given-name']
                    
                    if name and name.strip():
                        authors.append({
                            'name': name.strip(),
                            'scopus_id': author.get('authid', author.get('@auid', 'No ID'))
                        })
        
        elif 'dc:creator' in paper:
            creator = paper['dc:creator']
            if isinstance(creator, str):
                authors.append({'name': creator, 'scopus_id': 'No ID'})
            elif isinstance(creator, list):
                for c in creator:
                    name = c if isinstance(c, str) else str(c)
                    if name:
                        authors.append({'name': name, 'scopus_id': 'No ID'})
        
        return authors

    def get_papers(self, affiliation='main', search_query=None, max_papers=100):
        """Get Birmingham papers with complete author data"""
        
        # Build query
        if affiliation == 'all':
            all_ids = [aff_id for aff_id, _ in self.birmingham_affiliations.values()]
            aff_filter = '(' + ' OR '.join([f'AF-ID({aff_id})' for aff_id in all_ids]) + ')'
            aff_name = "All Birmingham affiliations"
        elif affiliation in self.birmingham_affiliations:
            aff_id, aff_name = self.birmingham_affiliations[affiliation]
            aff_filter = f'AF-ID({aff_id})'
        else:
            aff_id, aff_name = self.birmingham_affiliations['main']
            aff_filter = f'AF-ID({aff_id})'
        
        if search_query:
            if not any(keyword in search_query.upper() for keyword in ['TITLE-ABS-KEY', 'ALL']):
                search_query = f'TITLE-ABS-KEY("{search_query}")'
            final_query = f'({search_query}) AND {aff_filter}'
        else:
            final_query = aff_filter
        
        print(f"ğŸ›ï¸ Fixed Birmingham Pipeline")
        print(f"ğŸ“š Affiliation: {aff_name}")
        print(f"ğŸ” Query: {final_query}")
        
        # Get total count
        result = self.search_papers(final_query, count=1)
        if not result or 'search-results' not in result:
            print("âŒ Query failed")
            return []
        
        total = int(result['search-results'].get('opensearch:totalResults', 0))
        print(f"ğŸ“Š Found {total} total papers")
        
        if total == 0:
            return []
        
        # Download papers
        to_download = min(total, max_papers)
        print(f"ğŸ“¥ Downloading {to_download} papers with complete author data...")
        
        papers = []
        batch_size = 25
        
        for start in range(0, to_download, batch_size):
            current_batch = min(batch_size, to_download - start)
            print(f"ğŸ“¦ Getting papers {start + 1} to {start + current_batch}")
            
            result = self.search_papers(final_query, count=current_batch, start=start)
            
            if result and 'search-results' in result:
                entries = result['search-results'].get('entry', [])
                if entries:
                    papers.extend(entries)
                else:
                    print("âš ï¸ No entries in batch")
                    break
            else:
                print("âš ï¸ Error in batch")
                break
            
            # Progress
            if len(papers) % 50 == 0 and len(papers) > 0:
                print(f"âœ… Downloaded {len(papers)} papers so far...")
            
            # Rate limit
            time.sleep(1)
        
        print(f"âœ… Downloaded {len(papers)} papers total")
        return papers, final_query, aff_name

    def process_papers(self, papers):
        """Process papers with correct author extraction"""
        processed = []
        
        print(f"ğŸ”„ Processing {len(papers)} papers...")
        
        for i, paper in enumerate(papers):
            if i % 100 == 0 and i > 0:
                print(f"   Processed {i} papers...")
            
            # Extract authors using the working method
            authors = self.extract_authors_correctly(paper)
            
            # Handle author-count properly
            author_count_raw = paper.get('author-count', {})
            if isinstance(author_count_raw, dict):
                author_count = int(author_count_raw.get('$', 0))
            else:
                author_count = int(author_count_raw) if author_count_raw else len(authors)
            
            paper_info = {
                'scopus_id': paper.get('dc:identifier', '').replace('SCOPUS_ID:', ''),
                'title': paper.get('dc:title', 'No title'),
                'abstract': paper.get('dc:description', 'No abstract'),
                'publication_name': paper.get('prism:publicationName', 'Unknown'),
                'publication_date': paper.get('prism:coverDate', 'Unknown'),
                'citation_count': int(paper.get('citedby-count', 0)),
                'doi': paper.get('prism:doi', 'No DOI'),
                'eid': paper.get('eid', 'No EID'),
                'keywords': paper.get('authkeywords', 'No keywords'),
                'authors': authors,
                'author_count': author_count,
                'actual_author_count': len(authors)
            }
            
            # Extract affiliations
            affiliations = []
            if 'affiliation' in paper:
                affil_list = paper['affiliation'] if isinstance(paper['affiliation'], list) else [paper['affiliation']]
                for affil in affil_list:
                    if isinstance(affil, dict):
                        affiliations.append({
                            'name': affil.get('affilname', 'Unknown'),
                            'country': affil.get('affiliation-country', 'Unknown')
                        })
            paper_info['affiliations'] = affiliations
            
            # Extract subject areas
            subject_areas = []
            if 'subject-area' in paper:
                subject_list = paper['subject-area'] if isinstance(paper['subject-area'], list) else [paper['subject-area']]
                for subject in subject_list:
                    if isinstance(subject, dict):
                        subject_areas.append(subject.get('$', 'Unknown'))
            paper_info['subject_areas'] = subject_areas
            
            processed.append(paper_info)
        
        print(f"âœ… Processed {len(processed)} papers")
        return processed

    def save_data(self, data, query, aff_name):
        """Save data to files"""
        if not os.path.exists('scopus_outputs'):
            os.makedirs('scopus_outputs')
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = aff_name.lower().replace(' ', '_')[:20]
        
        # JSON
        json_file = f"scopus_outputs/birmingham_FIXED_{safe_name}_{len(data)}papers_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # CSV
        csv_file = f"scopus_outputs/birmingham_FIXED_{safe_name}_{len(data)}papers_{timestamp}.csv"
        flattened = []
        for paper in data:
            flat = paper.copy()
            author_names = [a['name'] for a in paper['authors'] if a['name'] != 'Unknown']
            flat['authors'] = '; '.join(author_names) if author_names else 'No authors'
            
            affil_names = [a['name'] for a in paper['affiliations'] if a['name'] != 'Unknown']
            flat['affiliations'] = '; '.join(affil_names) if affil_names else 'No affiliations'
            
            flat['subject_areas'] = '; '.join(paper['subject_areas']) if paper['subject_areas'] else 'No subjects'
            
            flattened.append(flat)
        
        df = pd.DataFrame(flattened)
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"ğŸ’¾ Saved:")
        print(f"  ğŸ“„ {json_file}")
        print(f"  ğŸ“Š {csv_file}")
        
        return json_file, csv_file

def main():
    print("ğŸš€ FIXED BIRMINGHAM PIPELINE")
    print("=" * 50)
    
    API_KEY = os.getenv('SCOPUS_API_KEY') or os.getenv('scopus_api_key')
    if not API_KEY:
        print("âŒ No API key found")
        return
    
    print(f"âœ… API Key: {API_KEY[:10]}...")
    
    pipeline = FixedBirminghamPipeline(API_KEY)
    
    print("\nğŸ“š Options:")
    print("1. main - University of Birmingham")
    print("2. business - Birmingham Business School") 
    print("3. medical_school - Birmingham Medical School")
    
    choice = input("\nChoice (default=main): ").strip()
    
    if choice == '2':
        affiliation = 'business'
    elif choice == '3':
        affiliation = 'medical_school'
    else:
        affiliation = 'main'
    
    search_query = input("\nSearch term (optional): ").strip()
    if not search_query:
        search_query = None
    
    max_input = input("\nMax papers (default=100): ").strip()
    max_papers = int(max_input) if max_input.isdigit() else 100
    
    try:
        # Get papers
        result = pipeline.get_papers(affiliation, search_query, max_papers)
        if not result:
            return
        
        papers, query, aff_name = result
        
        # Process with fixed author extraction
        processed = pipeline.process_papers(papers)
        
        # Save
        json_file, csv_file = pipeline.save_data(processed, query, aff_name)
        
        # Summary with author verification
        print(f"\nğŸ‰ FIXED PIPELINE COMPLETE!")
        print(f"ğŸ“Š Papers: {len(processed)}")
        
        if processed:
            # Check author extraction success
            papers_with_authors = [p for p in processed if len(p['authors']) > 0]
            multi_author_papers = [p for p in processed if len(p['authors']) > 1]
            
            print(f"ğŸ‘¥ Papers with authors: {len(papers_with_authors)}")
            print(f"ğŸ”¢ Papers with multiple authors: {len(multi_author_papers)}")
            
            # Show first paper authors
            if processed[0]['authors']:
                sample_authors = [a['name'] for a in processed[0]['authors'][:3]]
                print(f"âœ… First paper authors: {', '.join(sample_authors)}")
                print(f"ğŸ“Š Expected: {processed[0]['author_count']}, Actual: {processed[0]['actual_author_count']}")
            else:
                print("âŒ No authors extracted from first paper")
            
            total_citations = sum(p['citation_count'] for p in processed)
            print(f"ğŸ“ˆ Total citations: {total_citations}")
        
        print("âœ… Complete with full author data!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")

if __name__ == "__main__":
    main()